# üìè Guide du Context Size (Longueur de S√©quence)

## Qu'est-ce que le Context Size?

Le **context size** (ou `max_length`) d√©finit la **longueur maximale** d'une s√©quence (prompt + r√©ponse) pendant le fine-tuning, mesur√©e en **tokens**.

---

## üîß Configuration

### **Param√®tre dans les fichiers YAML**

```yaml
max_length: 4096  # Context size en tokens
```

**Fichiers concern√©s:**
- `configs/sft_lora_custom.yaml`
- `configs/sft_lora_combined.yaml`
- Tous les fichiers de configuration de fine-tuning

---

## üìä Valeurs Recommand√©es

| Context Size | Usage | Avantages | Inconv√©nients |
|--------------|-------|-----------|---------------|
| **1024** | R√©ponses courtes | Rapide, moins de VRAM | Tronque les longs textes |
| **2048** | Standard | Bon √©quilibre | Peut tronquer analyses longues |
| **4096** ‚úÖ | Recommand√© | Supporte analyses d√©taill√©es | Plus de VRAM n√©cessaire |
| **8192** | Tr√®s long | Aucune troncature | Beaucoup de VRAM, plus lent |

### **Configuration Actuelle: 4096 tokens**

C'est un bon compromis pour:
- ‚úÖ Analyses RGPD d√©taill√©es
- ‚úÖ Probl√®mes math√©matiques avec raisonnement √©tape par √©tape
- ‚úÖ R√©ponses compl√®tes sans troncature
- ‚úÖ Compatible avec la plupart des GPUs (24GB VRAM)

---

## üéØ Mod√®le de Base Apertus

Le mod√®le `swiss-ai/Apertus-8B-Instruct-2509` supporte:
- **Context window:** 8192 tokens (maximum)
- **Recommand√© pour fine-tuning:** 4096 tokens

---

## üíæ Impact sur la VRAM

| Context Size | Batch Size 4 | Batch Size 8 | VRAM Estim√©e |
|--------------|--------------|--------------|--------------|
| 1024 | ‚úÖ | ‚úÖ | ~12 GB |
| 2048 | ‚úÖ | ‚úÖ | ~16 GB |
| 4096 | ‚úÖ | ‚ö†Ô∏è | ~20-24 GB |
| 8192 | ‚ö†Ô∏è | ‚ùå | ~40+ GB |

**Note:** Avec `gradient_checkpointing: true`, la VRAM est r√©duite d'environ 30-40%.

---

## üîç V√©rifier la Longueur de Vos Donn√©es

### **Script de V√©rification**

```python
from transformers import AutoTokenizer
import json

tokenizer = AutoTokenizer.from_pretrained("swiss-ai/Apertus-8B-Instruct-2509")

# Charger votre dataset
with open("./data/combined_dataset/train.jsonl", "r") as f:
    lengths = []
    for line in f:
        data = json.loads(line)
        messages = data["messages"]
        
        # Appliquer le chat template
        text = tokenizer.apply_chat_template(messages, tokenize=False)
        tokens = tokenizer.encode(text)
        lengths.append(len(tokens))
    
    print(f"üìä Statistiques de longueur (tokens):")
    print(f"   - Min: {min(lengths)}")
    print(f"   - Max: {max(lengths)}")
    print(f"   - Moyenne: {sum(lengths)/len(lengths):.0f}")
    print(f"   - M√©diane: {sorted(lengths)[len(lengths)//2]}")
    
    # Pourcentage tronqu√© selon diff√©rents max_length
    for max_len in [1024, 2048, 4096, 8192]:
        truncated = sum(1 for l in lengths if l > max_len)
        pct = truncated / len(lengths) * 100
        print(f"   - Tronqu√©s avec max_length={max_len}: {truncated} ({pct:.1f}%)")
```

---

## ‚öôÔ∏è Ajuster le Context Size

### **Augmenter (pour textes plus longs)**

```yaml
max_length: 8192  # Double le context
```

**Attention:**
- ‚ö†Ô∏è N√©cessite plus de VRAM
- ‚ö†Ô∏è Training plus lent
- ‚ö†Ô∏è Peut n√©cessiter de r√©duire `per_device_train_batch_size`

### **R√©duire (pour √©conomiser VRAM)**

```yaml
max_length: 2048  # R√©duit de moiti√©
```

**Cons√©quences:**
- ‚úÖ Moins de VRAM
- ‚úÖ Training plus rapide
- ‚ùå Textes longs tronqu√©s

---

## üéØ Recommandations par Cas d'Usage

### **1. Dataset RGPD uniquement**
```yaml
max_length: 4096  # Analyses d√©taill√©es
```

### **2. Dataset Math uniquement**
```yaml
max_length: 2048  # Probl√®mes courts
```

### **3. Dataset Combin√© (RGPD + Math)**
```yaml
max_length: 4096  # Supporte les deux
```

### **4. GPU avec peu de VRAM (<16GB)**
```yaml
max_length: 2048
per_device_train_batch_size: 2
gradient_accumulation_steps: 16
gradient_checkpointing: true
```

### **5. GPU puissant (>40GB)**
```yaml
max_length: 8192
per_device_train_batch_size: 8
```

---

## üß™ Tester Diff√©rentes Valeurs

### **M√©thode 1: Tester avec un petit dataset**

```bash
# Cr√©er un petit subset pour test
head -n 100 ./data/combined_dataset/train.jsonl > ./data/test_subset/train.jsonl

# Tester avec max_length=2048
# Modifier le yaml, puis:
python sft_train.py configs/sft_lora_combined.yaml

# Observer la VRAM et la vitesse
```

### **M√©thode 2: Calculer la VRAM n√©cessaire**

**Formule approximative:**
```
VRAM (GB) ‚âà (model_size_GB √ó 1.5) + (batch_size √ó max_length √ó 0.002)
```

Pour Apertus-8B:
```
VRAM ‚âà (16 GB √ó 1.5) + (4 √ó 4096 √ó 0.002)
VRAM ‚âà 24 GB + 32.8 MB ‚âà 24 GB
```

---

## üìù Configuration Actuelle

### **`sft_lora_custom.yaml`**
```yaml
max_length: 4096
per_device_train_batch_size: 4
gradient_accumulation_steps: 8
gradient_checkpointing: true
```

**Effective batch size:** 4 √ó 8 = 32  
**VRAM estim√©e:** ~20-24 GB  
**Compatible avec:** RTX 3090, RTX 4090, A100 (24GB), L40S

### **`sft_lora_combined.yaml`**
```yaml
max_length: 4096
per_device_train_batch_size: 4
gradient_accumulation_steps: 8
gradient_checkpointing: true
```

**M√™me configuration** pour coh√©rence.

---

## ‚ö†Ô∏è Probl√®mes Courants

### **1. Out of Memory (OOM)**

**Sympt√¥me:** `CUDA out of memory`

**Solutions:**
```yaml
# Option 1: R√©duire max_length
max_length: 2048

# Option 2: R√©duire batch size
per_device_train_batch_size: 2
gradient_accumulation_steps: 16  # Compenser

# Option 3: Activer gradient checkpointing
gradient_checkpointing: true
```

### **2. Textes Tronqu√©s**

**Sympt√¥me:** R√©ponses incompl√®tes dans le dataset

**Solutions:**
```yaml
# Augmenter max_length
max_length: 8192

# Ou filtrer les exemples trop longs avant le training
```

### **3. Training Tr√®s Lent**

**Sympt√¥me:** <1 it/s

**Solutions:**
```yaml
# R√©duire max_length
max_length: 2048

# Ou activer packing (si support√©)
packing: true
```

---

## üîÑ Relation avec le Script de Test

Le script `test.py` utilise `max_new_tokens` pour la **g√©n√©ration**:

```python
max_new_tokens=4096  # Tokens g√©n√©r√©s (r√©ponse uniquement)
```

**Diff√©rence:**
- **`max_length` (training):** Prompt + R√©ponse
- **`max_new_tokens` (inference):** R√©ponse uniquement

**Coh√©rence recommand√©e:**
```yaml
# Training
max_length: 4096

# Test (dans test.py)
max_new_tokens: 4096  # Ou moins si prompt long
```

---

## üìä Monitoring

### **Pendant le Training**

Surveillez dans les logs:
```
[INFO] Truncated sequences: 0/100 (0.0%)
```

Si beaucoup de troncature:
- ‚úÖ Augmenter `max_length`
- ‚úÖ Ou filtrer les exemples longs

### **Apr√®s le Training**

Testez avec des questions longues:
```bash
./test.sh --model <chemin> --question "$(cat long_question.txt)"
```

---

## üí° Conseils

1. **Commencez avec 4096** - C'est un bon compromis
2. **V√©rifiez vos donn√©es** - Calculez la longueur moyenne
3. **Surveillez la VRAM** - Ajustez si OOM
4. **Testez** - Lancez un petit training pour valider
5. **Documentez** - Notez le max_length utilis√© pour chaque mod√®le

---

## üöÄ Quick Reference

| Objectif | max_length | Batch Size | VRAM |
|----------|------------|------------|------|
| **Rapide** | 2048 | 8 | ~16 GB |
| **Standard** ‚úÖ | 4096 | 4 | ~24 GB |
| **Maximum** | 8192 | 2 | ~40 GB |

**Configuration actuelle: Standard (4096 tokens)** ‚úÖ
