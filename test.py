#!/usr/bin/env python3
"""
Script unifi√© de test pour mod√®les Apertus
Usage:
    # Question libre sur mod√®le fine-tun√©
    python test.py --model <chemin> --question "Votre question"
    
    # Question libre sur mod√®le de base
    python test.py --base --question "Votre question"
    
    # Benchmark pr√©d√©fini sur mod√®le fine-tun√©
    python test.py --model <chemin> --benchmark
    
    # Comparaison BASE vs fine-tun√© avec question libre
    python test.py --model <chemin> --compare --question "Votre question"
    
    # Comparaison BASE vs fine-tun√© avec benchmark
    python test.py --model <chemin> --compare --benchmark
"""

import sys
import argparse
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from datetime import datetime

# Questions pr√©d√©finies pour le benchmark
BENCHMARK_QUESTIONS = [
    {
        "id": 1,
        "category": "Logique",
        "question": "Si tous les chats sont des animaux et que certains animaux volent, est-ce que certains chats volent? Explique ton raisonnement."
    },
    {
        "id": 2,
        "category": "Math√©matiques",
        "question": "Un train part de Paris √† 14h et roule √† 120 km/h. Un autre train part de Lyon (450 km de Paris) √† 14h30 et roule √† 100 km/h vers Paris. √Ä quelle heure et √† quelle distance de Paris se croiseront-ils?"
    },
    {
        "id": 3,
        "category": "Analyse Critique",
        "question": "Quels sont les avantages ET les inconv√©nients de l'intelligence artificielle dans le domaine m√©dical? Sois √©quilibr√© dans ton analyse."
    },
    {
        "id": 4,
        "category": "Cr√©ativit√©",
        "question": "√âcris le d√©but d'une histoire de science-fiction o√π un bug dans une IA de fine-tuning cr√©e accidentellement une conscience artificielle. Sois cr√©atif mais coh√©rent."
    },
    {
        "id": 5,
        "category": "Technique",
        "question": "Explique la diff√©rence entre LoRA et le fine-tuning complet d'un LLM. Quand utiliser l'un plut√¥t que l'autre?"
    },
    {
        "id": 6,
        "category": "√âthique",
        "question": "Une voiture autonome doit choisir entre percuter un groupe de 5 personnes ou d√©vier et tuer son unique passager. Analyse ce dilemme √©thique sans donner de r√©ponse simple."
    },
    {
        "id": 7,
        "category": "RGPD",
        "question": "Une entreprise europ√©enne utilise un mod√®le d'IA entra√Æn√© sur des donn√©es clients pour pr√©dire des comportements d'achat. Un client invoque son droit √† l'effacement (Article 17 RGPD). Quelles sont les obligations l√©gales de l'entreprise concernant: 1) les donn√©es d'entra√Ænement, 2) le mod√®le d√©j√† entra√Æn√©, 3) les pr√©dictions d√©j√† g√©n√©r√©es? Analyse les tensions entre droit √† l'oubli et impossibilit√© technique de 'd√©sapprendre' dans un mod√®le de ML."
    },
    {
        "id": 8,
        "category": "Programmation",
        "question": "√âcris une fonction Python qui trouve tous les nombres premiers jusqu'√† N en utilisant le crible d'√âratosth√®ne. Explique ton code."
    },
    {
        "id": 9,
        "category": "Multilinguisme",
        "question": "Explain in English the key differences between supervised and unsupervised learning, then give examples in French."
    }
]

BASE_MODEL_NAME = "swiss-ai/Apertus-8B-Instruct-2509"

# Syst√®me prompt utilis√© pendant le fine-tuning
DEFAULT_SYSTEM_PROMPT = "You are a helpful AI assistant specialized in data protection and privacy compliance."

def load_base_model():
    """Charge le mod√®le de base sans LoRA"""
    print("üîÑ Chargement du mod√®le de base...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        attn_implementation="flash_attention_2"
    )
    model.eval()
    return model, tokenizer

def load_finetuned_model(adapter_path):
    """Charge le mod√®le de base + adaptateurs LoRA"""
    print(f"üîÑ Chargement du mod√®le de base...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        attn_implementation="flash_attention_2"
    )
    
    print(f"üîÑ Chargement des adaptateurs LoRA depuis {adapter_path}...")
    model = PeftModel.from_pretrained(model, adapter_path)
    model.eval()
    return model, tokenizer

def generate_response(model, tokenizer, question, max_tokens=4096, system_prompt=None):
    """G√©n√®re une r√©ponse √† partir d'une question"""
    if system_prompt is None:
        system_prompt = DEFAULT_SYSTEM_PROMPT
    
    messages = [
        {
            "role": "system",
            "content": system_prompt
        },
        {
            "role": "user",
            "content": question
        }
    ]
    input_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # D√©coder seulement les nouveaux tokens g√©n√©r√©s (pas le prompt)
    generated_tokens = outputs[0][inputs.input_ids.shape[1]:]
    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    # Nettoyer les balises r√©siduelles
    if "<|im_end|>" in response:
        response = response.split("<|im_end|>")[0]
    
    response = response.strip()
    
    return response

def print_response(model_name, question, response, category=None):
    """Affiche une r√©ponse format√©e"""
    print("\n" + "=" * 80)
    if category:
        print(f"üìù CAT√âGORIE: {category}")
    print(f"ü§ñ MOD√àLE: {model_name}")
    print("=" * 80)
    print(f"\nüí¨ Question: {question}\n")
    print("‚îÄ" * 80)
    print(f"üìÑ R√©ponse:\n")
    print(response)
    print("\n" + "=" * 80)

def run_single_question(model, tokenizer, model_name, question):
    """Ex√©cute une question unique"""
    response = generate_response(model, tokenizer, question)
    print_response(model_name, question, response)

def run_benchmark(model, tokenizer, model_name, output_file=None):
    """Ex√©cute le benchmark complet"""
    results = []
    
    print("\n" + "‚ïê" * 80)
    print(f"üöÄ BENCHMARK COMPLET - {model_name}")
    print(f"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("‚ïê" * 80)
    
    for q in BENCHMARK_QUESTIONS:
        print(f"\n[{q['id']}/{len(BENCHMARK_QUESTIONS)}] {q['category']}...")
        response = generate_response(model, tokenizer, q['question'])
        results.append({
            'question': q,
            'response': response
        })
        print_response(model_name, q['question'], response, q['category'])
    
    # Sauvegarder si demand√©
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("‚ïê" * 80 + "\n")
            f.write(f"BENCHMARK - {model_name}\n")
            f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("‚ïê" * 80 + "\n\n")
            
            for r in results:
                q = r['question']
                f.write(f"\n{'‚îÄ' * 80}\n")
                f.write(f"TEST {q['id']}: {q['category']}\n")
                f.write(f"{'‚îÄ' * 80}\n")
                f.write(f"Question: {q['question']}\n\n")
                f.write(f"R√©ponse:\n{r['response']}\n\n")
        
        print(f"\n‚úÖ R√©sultats sauvegard√©s dans: {output_file}")
    
    return results

def run_comparison(base_model, base_tokenizer, ft_model, ft_tokenizer, ft_path, question=None, benchmark=False):
    """Compare mod√®le de base vs fine-tun√©"""
    print("\n" + "‚ïê" * 80)
    print("üî¨ MODE COMPARAISON")
    print(f"   Mod√®le A: {BASE_MODEL_NAME} (BASE)")
    print(f"   Mod√®le B: {ft_path} (FINE-TUN√â)")
    print("‚ïê" * 80)
    
    if benchmark:
        # Comparaison sur benchmark complet
        for q in BENCHMARK_QUESTIONS:
            print(f"\n{'‚ïê' * 80}")
            print(f"QUESTION {q['id']}: {q['category']}")
            print(f"{'‚ïê' * 80}")
            print(f"\nüí¨ {q['question']}\n")
            
            # Mod√®le de base
            print("‚îå" + "‚îÄ" * 78 + "‚îê")
            print("‚îÇ MOD√àLE A: BASE (sans fine-tuning)" + " " * 44 + "‚îÇ")
            print("‚îî" + "‚îÄ" * 78 + "‚îò")
            base_response = generate_response(base_model, base_tokenizer, q['question'])
            print(base_response)
            
            print("\n" + "‚îÄ" * 80 + "\n")
            
            # Mod√®le fine-tun√©
            print("‚îå" + "‚îÄ" * 78 + "‚îê")
            print("‚îÇ MOD√àLE B: FINE-TUN√â" + " " * 58 + "‚îÇ")
            print("‚îî" + "‚îÄ" * 78 + "‚îò")
            ft_response = generate_response(ft_model, ft_tokenizer, q['question'])
            print(ft_response)
            
            print("\n")
    else:
        # Comparaison sur question unique
        print(f"\nüí¨ Question: {question}\n")
        
        # Mod√®le de base
        print("‚îå" + "‚îÄ" * 78 + "‚îê")
        print("‚îÇ MOD√àLE A: BASE (sans fine-tuning)" + " " * 44 + "‚îÇ")
        print("‚îî" + "‚îÄ" * 78 + "‚îò")
        base_response = generate_response(base_model, base_tokenizer, question)
        print(base_response)
        
        print("\n" + "‚îÄ" * 80 + "\n")
        
        # Mod√®le fine-tun√©
        print("‚îå" + "‚îÄ" * 78 + "‚îê")
        print("‚îÇ MOD√àLE B: FINE-TUN√â" + " " * 58 + "‚îÇ")
        print("‚îî" + "‚îÄ" * 78 + "‚îò")
        ft_response = generate_response(ft_model, ft_tokenizer, question)
        print(ft_response)
    
    print("\n" + "‚ïê" * 80)
    print("‚úÖ Comparaison termin√©e!")
    print("‚ïê" * 80)

def main():
    parser = argparse.ArgumentParser(
        description="Script unifi√© de test pour mod√®les Apertus",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:

  # Question libre sur mod√®le fine-tun√©
  python test.py --model Apertus-FT/output/apertus_lora_custom_001 --question "Qu'est-ce que le RGPD?"
  
  # Question libre sur mod√®le de base
  python test.py --base --question "Qu'est-ce que le RGPD?"
  
  # Benchmark sur mod√®le fine-tun√©
  python test.py --model Apertus-FT/output/apertus_lora_custom_001 --benchmark
  
  # Comparaison avec question libre
  python test.py --model Apertus-FT/output/apertus_lora_custom_001 --compare --question "Qu'est-ce que le RGPD?"
  
  # Comparaison avec benchmark complet
  python test.py --model Apertus-FT/output/apertus_lora_custom_001 --compare --benchmark
  
  # Sauvegarder les r√©sultats
  python test.py --model Apertus-FT/output/apertus_lora_custom_001 --benchmark --output results.txt
        """
    )
    
    # Choix du mod√®le
    model_group = parser.add_mutually_exclusive_group(required=True)
    model_group.add_argument("--model", type=str, help="Chemin vers le mod√®le fine-tun√©")
    model_group.add_argument("--base", action="store_true", help="Utiliser le mod√®le de base")
    
    # Type de test
    test_group = parser.add_mutually_exclusive_group(required=True)
    test_group.add_argument("--question", type=str, help="Question libre √† poser")
    test_group.add_argument("--benchmark", action="store_true", help="Lancer le benchmark pr√©d√©fini (9 questions)")
    
    # Options
    parser.add_argument("--compare", action="store_true", help="Comparer BASE vs mod√®le fine-tun√©")
    parser.add_argument("--output", type=str, help="Fichier de sortie pour sauvegarder les r√©sultats")
    parser.add_argument("--max-tokens", type=int, default=4096, help="Nombre max de tokens √† g√©n√©rer (d√©faut: 4096)")
    parser.add_argument("--system-prompt", type=str, help=f"Syst√®me prompt personnalis√© (d√©faut: '{DEFAULT_SYSTEM_PROMPT[:50]}...')")
    
    args = parser.parse_args()
    
    # Validation
    if args.compare and args.base:
        parser.error("--compare n√©cessite --model (pas --base)")
    
    if args.output and not args.benchmark:
        parser.error("--output n√©cessite --benchmark")
    
    try:
        # Mode comparaison
        if args.compare:
            base_model, base_tokenizer = load_base_model()
            ft_model, ft_tokenizer = load_finetuned_model(args.model)
            
            run_comparison(
                base_model, base_tokenizer,
                ft_model, ft_tokenizer,
                args.model,
                question=args.question,
                benchmark=args.benchmark
            )
        
        # Mode simple (un seul mod√®le)
        else:
            if args.base:
                model, tokenizer = load_base_model()
                model_name = f"{BASE_MODEL_NAME} (BASE)"
            else:
                model, tokenizer = load_finetuned_model(args.model)
                model_name = args.model
            
            if args.benchmark:
                output_file = args.output
                if output_file is None and not args.base:
                    # G√©n√©rer nom de fichier automatique
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    output_file = f"benchmark_{timestamp}.txt"
                
                run_benchmark(model, tokenizer, model_name, output_file)
            else:
                run_single_question(model, tokenizer, model_name, args.question)
    
    except Exception as e:
        print(f"\n‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
