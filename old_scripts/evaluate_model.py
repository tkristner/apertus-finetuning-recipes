#!/usr/bin/env python3
"""
Script d'√©valuation automatique d'un mod√®le par rapport aux r√©ponses de r√©f√©rence.
Usage: python evaluate_model.py <chemin_mod√®le> [--output rapport.txt]
"""

import sys
import argparse
from datetime import datetime

# Crit√®res d'√©valuation pour chaque test
EVALUATION_CRITERIA = {
    "test1_logique": {
        "name": "Raisonnement Logique",
        "weight": 10,
        "keywords_positive": ["non", "invalide", "syllogisme", "ne peut pas conclure", "erreur logique"],
        "keywords_negative": ["oui", "certains chats volent"],
        "max_score": 10
    },
    "test2_maths": {
        "name": "Math√©matiques",
        "weight": 10,
        "keywords_positive": ["16h", "272", "390", "220", "vitesse", "distance"],
        "keywords_negative": ["15h", "17h", "100 km", "500 km"],
        "max_score": 10
    },
    "test3_analyse": {
        "name": "Analyse Critique IA M√©dicale",
        "weight": 15,
        "keywords_positive": ["avantage", "inconv√©nient", "biais", "diagnostic", "√©thique", "rgpd", "responsabilit√©"],
        "keywords_negative": ["seulement positif", "uniquement n√©gatif"],
        "max_score": 15
    },
    "test4_creativite": {
        "name": "Cr√©ativit√© Science-Fiction",
        "weight": 10,
        "keywords_positive": ["bug", "conscience", "√©mergence", "mod√®le", "entra√Ænement", "question"],
        "keywords_negative": [],
        "max_score": 10
    },
    "test5_technique": {
        "name": "LoRA vs Fine-tuning",
        "weight": 15,
        "keywords_positive": ["lora", "param√®tres", "m√©moire", "adaptateur", "rank", "efficacit√©", "cas d'usage"],
        "keywords_negative": [],
        "max_score": 15
    },
    "test6_ethique": {
        "name": "Dilemme √âthique",
        "weight": 15,
        "keywords_positive": ["utilitarisme", "d√©ontologie", "dilemme", "complexe", "pas de r√©ponse simple", "perspective"],
        "keywords_negative": ["r√©ponse simple", "√©vident"],
        "max_score": 15
    },
    "test7_rgpd": {
        "name": "RGPD et ML",
        "weight": 15,
        "keywords_positive": ["article 17", "effacement", "donn√©es personnelles", "mod√®le", "machine unlearning", "zone grise", "dpo"],
        "keywords_negative": [],
        "max_score": 15
    },
    "test8_code": {
        "name": "Programmation",
        "weight": 5,
        "keywords_positive": ["def", "for", "range", "crible", "premier", "return"],
        "keywords_negative": [],
        "max_score": 5
    },
    "test9_multilingue": {
        "name": "Multilinguisme",
        "weight": 5,
        "keywords_positive": ["supervised", "unsupervised", "supervis√©", "non supervis√©", "example"],
        "keywords_negative": [],
        "max_score": 5
    }
}

def print_evaluation_guide():
    """Affiche le guide d'√©valuation"""
    print("=" * 80)
    print("üìã GUIDE D'√âVALUATION MANUELLE")
    print("=" * 80)
    print()
    print("Pour chaque test, √©valuez la r√©ponse du mod√®le selon les crit√®res suivants:")
    print()
    
    for test_id, criteria in EVALUATION_CRITERIA.items():
        print(f"{'‚îÄ' * 80}")
        print(f"üîπ {criteria['name']} (Max: {criteria['max_score']} points)")
        print(f"{'‚îÄ' * 80}")
        print(f"Poids: {criteria['weight']}%")
        print(f"Mots-cl√©s attendus: {', '.join(criteria['keywords_positive'][:5])}")
        print()
    
    print("=" * 80)
    print()
    print("üìä GRILLE DE NOTATION:")
    print("  - 90-100: Expert (r√©ponse compl√®te, nuanc√©e, pr√©cise)")
    print("  - 75-89:  Avanc√© (bonne r√©ponse avec quelques manques)")
    print("  - 60-74:  Interm√©diaire (r√©ponse correcte mais superficielle)")
    print("  - 45-59:  D√©butant (r√©ponse partielle ou impr√©cise)")
    print("  - <45:    Insuffisant (r√©ponse incorrecte ou hors-sujet)")
    print()
    print("=" * 80)

def analyze_response(response, criteria):
    """Analyse basique d'une r√©ponse (d√©tection de mots-cl√©s)"""
    response_lower = response.lower()
    
    # Compter les mots-cl√©s positifs
    positive_count = sum(1 for kw in criteria['keywords_positive'] if kw in response_lower)
    
    # Compter les mots-cl√©s n√©gatifs
    negative_count = sum(1 for kw in criteria['keywords_negative'] if kw in response_lower)
    
    # Score basique (√† affiner manuellement)
    base_score = min(criteria['max_score'], (positive_count / max(len(criteria['keywords_positive']), 1)) * criteria['max_score'])
    
    # P√©nalit√© pour mots-cl√©s n√©gatifs
    penalty = negative_count * 2
    
    estimated_score = max(0, base_score - penalty)
    
    return {
        'estimated_score': estimated_score,
        'positive_matches': positive_count,
        'negative_matches': negative_count,
        'confidence': 'low'  # Toujours basse car analyse automatique limit√©e
    }

def generate_evaluation_template(model_path):
    """G√©n√®re un template d'√©valuation √† remplir manuellement"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"evaluation_template_{timestamp}.txt"
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write(f"√âVALUATION MANUELLE DU MOD√àLE: {model_path}\n")
        f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("INSTRUCTIONS:\n")
        f.write("1. Ex√©cutez: ./benchmark_model.sh " + model_path + "\n")
        f.write("2. Lisez les r√©ponses du mod√®le\n")
        f.write("3. Comparez avec reference_answers.md\n")
        f.write("4. Remplissez les scores ci-dessous (0-max)\n")
        f.write("5. Calculez le score total\n\n")
        
        f.write("=" * 80 + "\n\n")
        
        total_max = 0
        for test_id, criteria in EVALUATION_CRITERIA.items():
            f.write(f"{'‚îÄ' * 80}\n")
            f.write(f"TEST: {criteria['name']}\n")
            f.write(f"{'‚îÄ' * 80}\n")
            f.write(f"Poids: {criteria['weight']}% | Score max: {criteria['max_score']}\n\n")
            f.write("Crit√®res √† √©valuer:\n")
            for kw in criteria['keywords_positive'][:5]:
                f.write(f"  - {kw}\n")
            f.write("\n")
            f.write(f"SCORE: _____ / {criteria['max_score']}\n")
            f.write("COMMENTAIRES:\n\n\n\n")
            total_max += criteria['max_score']
        
        f.write("=" * 80 + "\n")
        f.write(f"SCORE TOTAL: _____ / {total_max}\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("NIVEAU:\n")
        f.write("[ ] 90-100: Expert\n")
        f.write("[ ] 75-89:  Avanc√©\n")
        f.write("[ ] 60-74:  Interm√©diaire\n")
        f.write("[ ] 45-59:  D√©butant\n")
        f.write("[ ] <45:    Insuffisant\n\n")
        
        f.write("OBSERVATIONS G√âN√âRALES:\n")
        f.write("\n" * 5)
        
        f.write("POINTS FORTS:\n")
        f.write("\n" * 3)
        
        f.write("POINTS √Ä AM√âLIORER:\n")
        f.write("\n" * 3)
    
    return filename

def main():
    parser = argparse.ArgumentParser(description="√âvaluation d'un mod√®le fine-tun√©")
    parser.add_argument("model_path", help="Chemin vers le mod√®le √† √©valuer")
    parser.add_argument("--guide", action="store_true", help="Afficher le guide d'√©valuation")
    parser.add_argument("--template", action="store_true", help="G√©n√©rer un template d'√©valuation")
    
    args = parser.parse_args()
    
    if args.guide:
        print_evaluation_guide()
        return
    
    if args.template:
        filename = generate_evaluation_template(args.model_path)
        print(f"‚úÖ Template d'√©valuation g√©n√©r√©: {filename}")
        print()
        print("üìù Prochaines √©tapes:")
        print(f"   1. Ex√©cutez: ./benchmark_model.sh {args.model_path}")
        print(f"   2. Ouvrez: {filename}")
        print(f"   3. Comparez avec: reference_answers.md")
        print(f"   4. Remplissez les scores manuellement")
        return
    
    # Par d√©faut, afficher le guide
    print_evaluation_guide()
    print()
    print("üí° Pour g√©n√©rer un template d'√©valuation:")
    print(f"   python evaluate_model.py {args.model_path} --template")

if __name__ == "__main__":
    main()
