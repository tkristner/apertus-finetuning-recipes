# üß™ Guide de Test et √âvaluation des Mod√®les

Ce guide explique comment tester et comparer vos mod√®les fine-tun√©s avec le mod√®le de base.

---

## üìã Scripts Disponibles

### 1. **Test d'un Mod√®le Fine-tun√©**

```bash
python test_model.py <chemin_mod√®le> "Votre question"
```

**Exemple:**
```bash
python test_model.py Apertus-FT/output/apertus_lora_custom_001 "Qu'est-ce que le RGPD?"
```

### 2. **Test du Mod√®le de Base (sans fine-tuning)**

```bash
python test_base_model.py "Votre question"
```

**Exemple:**
```bash
python test_base_model.py "Qu'est-ce que le RGPD?"
```

### 3. **Tests Interactifs Challengeants**

```bash
./quick_test.sh [chemin_mod√®le]
```

**7 tests avec pauses:**
- Raisonnement logique
- Math√©matiques
- Analyse critique
- Cr√©ativit√©
- Technique (LoRA)
- √âthique
- RGPD (niveau DPO)

**Exemple:**
```bash
./quick_test.sh Apertus-FT/output/apertus_lora_custom_001
```

### 4. **Benchmark Automatique Complet**

```bash
./benchmark_model.sh [chemin_mod√®le]
```

**9 tests automatiques + sauvegarde:**
```bash
./benchmark_model.sh Apertus-FT/output/apertus_lora_custom_001
# G√©n√®re: benchmark_results_YYYYMMDD_HHMMSS.txt
```

---

## üî¨ Comparaison de Mod√®les

### **Comparer avec le Mod√®le de Base (RECOMMAND√â)**

```bash
./compare_models.sh BASE <chemin_mod√®le_finetun√©>
```

**Exemple:**
```bash
./compare_models.sh BASE Apertus-FT/output/apertus_lora_custom_001
```

Cela compare:
- **Mod√®le A:** `swiss-ai/Apertus-8B-Instruct-2509` (BASE - sans fine-tuning)
- **Mod√®le B:** Votre mod√®le fine-tun√©

### **Comparer Deux Mod√®les Fine-tun√©s**

```bash
./compare_models.sh <mod√®le1> <mod√®le2>
```

**Exemple:**
```bash
./compare_models.sh Apertus-FT/output/apertus_lora Apertus-FT/output/apertus_lora_custom_001
```

### **Comparaison par D√©faut**

Sans arguments, compare le mod√®le de base avec le dernier fine-tuning:
```bash
./compare_models.sh
# √âquivalent √†: ./compare_models.sh BASE Apertus-FT/output/apertus_lora_custom_001
```

---

## üìä √âvaluation avec R√©f√©rentiel

### **1. G√©n√©rer un Template d'√âvaluation**

```bash
python evaluate_model.py <chemin_mod√®le> --template
```

**Exemple:**
```bash
python evaluate_model.py Apertus-FT/output/apertus_lora_custom_001 --template
# G√©n√®re: evaluation_template_YYYYMMDD_HHMMSS.txt
```

### **2. Afficher le Guide d'√âvaluation**

```bash
python evaluate_model.py <chemin_mod√®le> --guide
```

### **3. Workflow Complet d'√âvaluation**

```bash
# √âtape 1: Lancer le benchmark
./benchmark_model.sh Apertus-FT/output/apertus_lora_custom_001

# √âtape 2: G√©n√©rer le template d'√©valuation
python evaluate_model.py Apertus-FT/output/apertus_lora_custom_001 --template

# √âtape 3: Ouvrir les 3 fichiers
# - benchmark_results_*.txt (r√©ponses du mod√®le)
# - reference_answers.md (r√©ponses attendues)
# - evaluation_template_*.txt (grille √† remplir)

# √âtape 4: Comparer et noter manuellement
```

---

## üéØ Cas d'Usage Typiques

### **Cas 1: √âvaluer l'Impact du Fine-tuning**

```bash
# Comparer BASE vs votre fine-tuning
./compare_models.sh BASE Apertus-FT/output/apertus_lora_custom_001
```

**Questions √† se poser:**
- Le mod√®le fine-tun√© est-il meilleur sur les questions sp√©cifiques?
- A-t-il perdu des capacit√©s g√©n√©rales? (catastrophic forgetting)
- Les r√©ponses sont-elles plus pr√©cises/d√©taill√©es?

### **Cas 2: Comparer Deux Configurations de Fine-tuning**

```bash
# Ancien (rank 16) vs nouveau (rank 32)
./compare_models.sh \
    Apertus-FT/output/apertus_lora \
    Apertus-FT/output/apertus_lora_custom_001
```

**Questions √† se poser:**
- Quelle configuration donne les meilleures r√©ponses?
- Le rank plus √©lev√© am√©liore-t-il vraiment la qualit√©?
- Y a-t-il de l'overfitting?

### **Cas 3: Test Rapide d'un Nouveau Mod√®le**

```bash
# Test personnalis√©
python test_model.py Apertus-FT/output/nouveau_modele "Question sp√©cifique √† votre domaine"
```

### **Cas 4: √âvaluation Compl√®te pour Production**

```bash
# 1. Benchmark complet
./benchmark_model.sh Apertus-FT/output/apertus_lora_custom_001

# 2. Comparaison avec BASE
./compare_models.sh BASE Apertus-FT/output/apertus_lora_custom_001

# 3. √âvaluation formelle
python evaluate_model.py Apertus-FT/output/apertus_lora_custom_001 --template

# 4. Remplir la grille d'√©valuation manuellement
```

---

## üìà Grille d'√âvaluation

| Test | Poids | Points Max | Crit√®res |
|------|-------|------------|----------|
| 1. Logique | 10% | 10 | Raisonnement valide |
| 2. Maths | 10% | 10 | Calculs corrects |
| 3. Analyse | 15% | 15 | √âquilibre + nuances |
| 4. Cr√©ativit√© | 10% | 10 | Originalit√© + coh√©rence |
| 5. Technique | 15% | 15 | Pr√©cision + cas d'usage |
| 6. √âthique | 15% | 15 | Perspectives multiples |
| 7. RGPD | 15% | 15 | Expertise juridique + technique |
| 8. Code | 5% | 5 | Fonctionnel + optimis√© |
| 9. Multilingue | 5% | 5 | Qualit√© linguistique |
| **TOTAL** | **100%** | **100** | |

**Niveaux de Performance:**
- **90-100:** Expert - Pr√™t pour production
- **75-89:** Avanc√© - Bon pour la plupart des cas
- **60-74:** Interm√©diaire - N√©cessite am√©liorations
- **45-59:** D√©butant - R√©entra√Ænement recommand√©
- **<45:** Insuffisant - Revoir la strat√©gie de fine-tuning

---

## üîç Interpr√©tation des R√©sultats

### **Signes d'un Bon Fine-tuning:**
‚úÖ Am√©lioration sur les t√¢ches cibl√©es
‚úÖ Pr√©servation des capacit√©s g√©n√©rales
‚úÖ R√©ponses plus structur√©es et d√©taill√©es
‚úÖ Meilleure adh√©rence au format attendu
‚úÖ R√©duction des hallucinations

### **Signes de Probl√®mes:**
‚ùå Catastrophic forgetting (perte de connaissances g√©n√©rales)
‚ùå Overfitting (r√©ponses trop sp√©cifiques/r√©p√©titives)
‚ùå D√©gradation de la qualit√© linguistique
‚ùå Augmentation des hallucinations
‚ùå R√©ponses moins coh√©rentes

### **Actions Correctives:**

**Si catastrophic forgetting:**
- R√©duire le learning rate
- Augmenter le warmup
- Utiliser LoRA avec rank plus faible
- M√©langer donn√©es g√©n√©rales dans le dataset

**Si overfitting:**
- Augmenter le dropout
- R√©duire le nombre d'epochs
- Augmenter la taille du dataset
- Ajouter de la diversit√© dans les donn√©es

**Si qualit√© insuffisante:**
- Augmenter le LoRA rank
- Augmenter le nombre d'epochs
- Am√©liorer la qualit√© du dataset
- Consid√©rer le full fine-tuning

---

## üìÅ Fichiers G√©n√©r√©s

```
apertus-finetuning-recipes/
‚îú‚îÄ‚îÄ benchmark_results_YYYYMMDD_HHMMSS.txt    # R√©sultats des tests
‚îú‚îÄ‚îÄ evaluation_template_YYYYMMDD_HHMMSS.txt  # Grille d'√©valuation
‚îî‚îÄ‚îÄ reference_answers.md                      # R√©ponses de r√©f√©rence
```

---

## üí° Conseils

1. **Toujours comparer avec BASE** pour mesurer l'impact r√©el du fine-tuning
2. **Tester sur des questions hors dataset** pour v√©rifier la g√©n√©ralisation
3. **Documenter les scores** pour suivre l'√©volution entre versions
4. **Partager les r√©sultats** avec l'√©quipe pour d√©cisions collectives
5. **It√©rer rapidement** avec les tests rapides avant le benchmark complet

---

## üöÄ Exemples Complets

### **Exemple 1: Premier Fine-tuning**

```bash
# 1. Test rapide
python test_model.py Apertus-FT/output/mon_premier_ft "Bonjour, qui es-tu?"

# 2. Comparaison avec BASE
./compare_models.sh BASE Apertus-FT/output/mon_premier_ft

# 3. Si satisfait, benchmark complet
./benchmark_model.sh Apertus-FT/output/mon_premier_ft
```

### **Exemple 2: Optimisation It√©rative**

```bash
# Version 1 (rank 16, lr 2e-4)
./benchmark_model.sh Apertus-FT/output/v1_rank16

# Version 2 (rank 32, lr 5e-5)
./benchmark_model.sh Apertus-FT/output/v2_rank32

# Comparaison directe
./compare_models.sh Apertus-FT/output/v1_rank16 Apertus-FT/output/v2_rank32
```

### **Exemple 3: Validation Finale**

```bash
# Benchmark complet
./benchmark_model.sh Apertus-FT/output/final_model

# √âvaluation formelle
python evaluate_model.py Apertus-FT/output/final_model --template

# Comparaison avec BASE
./compare_models.sh BASE Apertus-FT/output/final_model

# D√©cision: d√©ployer si score > 75
```

---

## üìû Support

Pour toute question sur les tests et l'√©valuation, consultez:
- `reference_answers.md` - R√©ponses attendues d√©taill√©es
- `CUSTOM_DATASET_GUIDE.md` - Guide du dataset
- `README.md` - Documentation g√©n√©rale
