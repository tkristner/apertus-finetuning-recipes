# üìä Guide du Syst√®me de Logging Automatis√©

Le syst√®me de logging a √©t√© enti√®rement automatis√© pour capturer les m√©triques de training, g√©n√©rer les graphiques et archiver les param√®tres sans intervention manuelle.

## üéØ Fonctionnalit√©s

### ‚úÖ Automatique
- ‚úÖ **Capture automatique** des logs de training en temps r√©el
- ‚úÖ **Sauvegarde automatique** des param√®tres au d√©marrage
- ‚úÖ **G√©n√©ration automatique** des graphiques √† la fin
- ‚úÖ **R√©sum√© textuel** du training avec statistiques
- ‚úÖ **Archivage de la config YAML** utilis√©e

### üìÅ Fichiers G√©n√©r√©s

√Ä la fin du fine-tuning, le r√©pertoire de sortie contient automatiquement :

```
Apertus-FT/output/apertus_lora_combined_20251107_123456/
‚îú‚îÄ‚îÄ config_backup/
‚îÇ   ‚îî‚îÄ‚îÄ sft_lora_combined.yaml        # Config YAML utilis√©e
‚îú‚îÄ‚îÄ training_parameters.json          # Tous les param√®tres de training
‚îú‚îÄ‚îÄ training_logs.jsonl               # Logs bruts (format ligne par ligne)
‚îú‚îÄ‚îÄ training_logs.json                # Logs en JSON complet
‚îú‚îÄ‚îÄ training_config.json              # Config d√©taill√©e du trainer
‚îú‚îÄ‚îÄ training_metrics.png              # Graphiques complets (6 m√©triques)
‚îú‚îÄ‚îÄ training_loss.png                 # Graphique focus sur la loss
‚îú‚îÄ‚îÄ training_summary.txt              # R√©sum√© textuel
‚îî‚îÄ‚îÄ [mod√®le et autres fichiers...]
```

## üöÄ Utilisation

### Lancement Standard

```bash
# Activer l'environnement
source apertus/bin/activate

# Lancer le fine-tuning (le logging est automatique)
apertus sft configs/sft_lora_combined.yaml
```

**Aucune action manuelle n√©cessaire !** Le syst√®me :
1. ‚úÖ Cr√©e un dossier avec timestamp
2. ‚úÖ Sauvegarde les param√®tres au d√©marrage
3. ‚úÖ Capture les logs en temps r√©el
4. ‚úÖ G√©n√®re les graphiques √† la fin
5. ‚úÖ Cr√©e un r√©sum√© textuel

### Sortie Console

Le syst√®me affiche des messages clairs √† chaque √©tape :

```
üìÅ Output directory: Apertus-FT/output/apertus_lora_combined_20251107_123456
‚úÖ Config YAML sauvegard√©e: .../config_backup/sft_lora_combined.yaml
‚úÖ Param√®tres sauvegard√©s: .../training_parameters.json

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üöÄ D√âBUT DU FINE-TUNING
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìÅ Output dir: Apertus-FT/output/apertus_lora_combined_20251107_123456
üïê Start time: 2025-11-07 12:34:56
‚úÖ Config sauvegard√©e: .../training_config.json

[... training en cours ...]

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚úÖ FIN DU FINE-TUNING
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üïê End time: 2025-11-07 14:12:34
‚è±Ô∏è  Duration: 1:37:38
üìä Total steps: 1938

‚úÖ Logs sauvegard√©s: .../training_logs.jsonl
‚úÖ Logs sauvegard√©s: .../training_logs.json

üìà G√©n√©ration des graphiques...
  ‚úì .../training_metrics.png
  ‚úì .../training_loss.png
‚úÖ Graphiques g√©n√©r√©s

‚úÖ R√©sum√© sauvegard√©: .../training_summary.txt
```

## üìä Visualisation des Graphiques

### Automatique (Recommand√©)

Les graphiques sont g√©n√©r√©s automatiquement √† la fin du training.

### Manuel (Si N√©cessaire)

Si vous voulez reg√©n√©rer les graphiques :

```bash
# Utiliser le dernier dossier de training
python plot_training_logs.py

# Sp√©cifier un dossier sp√©cifique
python plot_training_logs.py --output-dir Apertus-FT/output/apertus_lora_combined_20251107_123456

# Utiliser un fichier de log sp√©cifique (ancien format)
python plot_training_logs.py --log-file training_loss_logs.txt
```

## üìà M√©triques Captur√©es

### Training Loss
- √âvolution par step
- √âvolution par epoch
- Statistiques (min, max, finale, am√©lioration)

### Token Accuracy
- Pr√©cision moyenne des tokens
- √âvolution au cours du training

### Learning Rate
- Schedule complet
- Valeur maximale et finale

### Gradient Norm
- Stabilit√© du gradient
- D√©tection de valeurs anormales

### Entropy
- Confiance du mod√®le
- √âvolution de l'incertitude

## üóÇÔ∏è Format des Fichiers

### training_parameters.json

```json
{
  "timestamp": "20251107_123456",
  "model": {
    "name_or_path": "swiss-ai/Apertus-8B-Instruct-2509",
    "dtype": "torch.bfloat16",
    "attn_implementation": "flash_attention_2"
  },
  "dataset": {
    "name": "./data/custom_mix_dataset_hf",
    "train_split": "train",
    "test_split": "test"
  },
  "training": {
    "learning_rate": 7e-05,
    "num_train_epochs": 1.0,
    "per_device_train_batch_size": 8,
    "gradient_accumulation_steps": 4,
    ...
  }
}
```

### training_logs.jsonl

Format ligne par ligne (un JSON par ligne) :

```jsonl
{"step": 1, "epoch": 0.0, "loss": 1.7988, "grad_norm": 0.39, "learning_rate": 0.0, ...}
{"step": 2, "epoch": 0.01, "loss": 1.7622, "grad_norm": 0.40, "learning_rate": 2.5e-07, ...}
...
```

### training_summary.txt

R√©sum√© textuel lisible :

```
======================================================================
R√âSUM√â DU FINE-TUNING
======================================================================

üìÖ D√©but:     2025-11-07 12:34:56
üìÖ Fin:       2025-11-07 14:12:34
‚è±Ô∏è  Dur√©e:     1:37:38

üìä M√âTRIQUES
----------------------------------------------------------------------
Total steps:          1938

Loss:
  - Initiale:         1.7988
  - Finale:           1.1013
  - Minimale:         0.9683
  - Maximale:         1.7988
  - Am√©lioration:     38.78%

Accuracy:
  - Initiale:         0.6046
  - Finale:           0.7079
  - Am√©lioration:     17.08%
...
```

## üîß Personnalisation

### Modifier les M√©triques Captur√©es

√âditez `training_logger.py`, m√©thode `on_log()` :

```python
def on_log(self, args, state, control, logs=None, **kwargs):
    if logs is None:
        return

    if 'loss' in logs and state.global_step > 0:
        log_entry = {
            'step': state.global_step,
            'epoch': state.epoch,
            'loss': logs.get('loss', None),
            # Ajouter vos m√©triques personnalis√©es ici
            'custom_metric': logs.get('custom_metric', None),
        }
```

### Ajouter des Graphiques Personnalis√©s

√âditez `training_logger.py`, m√©thode `_generate_plots()` :

```python
def _generate_plots(self):
    # ... code existant ...

    # Ajouter votre graphique personnalis√©
    self._plot_custom_metric(data)
```

## üêõ Troubleshooting

### Erreur : "ModuleNotFoundError: No module named 'training_logger'"

**Solution** : Assurez-vous d'√™tre dans le bon r√©pertoire :
```bash
cd /path/to/apertus-finetuning-recipes
source apertus/bin/activate
```

### Erreur : "ModuleNotFoundError: No module named 'matplotlib'"

**Solution** : Matplotlib devrait d√©j√† √™tre install√©. Si ce n'est pas le cas :
```bash
source apertus/bin/activate
uv pip install matplotlib
```

### Les graphiques ne sont pas g√©n√©r√©s

**V√©rifications** :
1. Le training s'est-il termin√© normalement ?
2. Y a-t-il un fichier `training_logs.jsonl` ?
3. Y a-t-il des donn√©es dans le fichier ?

```bash
# V√©rifier le dernier dossier
ls -lh Apertus-FT/output/apertus_lora_combined_*/training_logs.jsonl

# Compter les lignes
wc -l Apertus-FT/output/apertus_lora_combined_*/training_logs.jsonl
```

### Reg√©n√©rer les graphiques manuellement

```bash
python plot_training_logs.py --output-dir Apertus-FT/output/apertus_lora_combined_20251107_123456
```

## üí° Conseils

### Pour le D√©bogage

1. **V√©rifier les logs en temps r√©el** :
   ```bash
   tail -f Apertus-FT/output/apertus_lora_combined_*/training_logs.jsonl
   ```

2. **Comparer deux trainings** :
   ```bash
   python plot_training_logs.py --output-dir Apertus-FT/output/apertus_lora_combined_20251107_123456
   python plot_training_logs.py --output-dir Apertus-FT/output/apertus_lora_combined_20251107_145678
   ```

3. **Analyser les param√®tres** :
   ```bash
   cat Apertus-FT/output/apertus_lora_combined_*/training_parameters.json | jq .
   ```

### Pour l'Archivage

Tous les fichiers n√©cessaires pour reproduire le training sont sauvegard√©s :

```bash
# Cr√©er une archive d'un training
cd Apertus-FT/output
tar -czf apertus_lora_combined_20251107_123456.tar.gz apertus_lora_combined_20251107_123456/

# Extraire plus tard
tar -xzf apertus_lora_combined_20251107_123456.tar.gz
```

## üìö Fichiers Associ√©s

- **`training_logger.py`** - Callback principal de logging
- **`plot_training_logs.py`** - Script de g√©n√©ration de graphiques
- **`sft_train.py`** - Script de training modifi√©

## üîÑ Migration depuis l'Ancien Syst√®me

### Ancien Workflow (Manuel)

```bash
# 1. Copier manuellement les logs de la console
# 2. Coller dans un fichier .txt
# 3. Modifier plot_training_logs.py avec le bon chemin
# 4. Ex√©cuter plot_training_logs.py
python plot_training_logs.py
```

### Nouveau Workflow (Automatique)

```bash
# 1. Lancer le training
apertus sft configs/sft_lora_combined.yaml

# C'est tout ! Les logs, graphiques et r√©sum√©s sont g√©n√©r√©s automatiquement
```

### Compatibilit√©

Le nouveau syst√®me est **r√©tro-compatible** avec l'ancien format :

```bash
# Utiliser l'ancien fichier de logs
python plot_training_logs.py --log-file training_loss_logs.txt
```

## ‚úÖ Checklist Post-Training

Apr√®s chaque fine-tuning, v√©rifiez que vous avez :

- [ ] `training_parameters.json` - Param√®tres sauvegard√©s
- [ ] `config_backup/` - Config YAML archiv√©e
- [ ] `training_logs.jsonl` - Logs bruts
- [ ] `training_logs.json` - Logs JSON
- [ ] `training_metrics.png` - Graphiques complets
- [ ] `training_loss.png` - Graphique loss
- [ ] `training_summary.txt` - R√©sum√©

Si un fichier manque, utilisez :
```bash
python plot_training_logs.py --output-dir <output_dir>
```

---

**Version** : 2.0 - Logging Automatis√©
**Derni√®re mise √† jour** : 2025-11-07
