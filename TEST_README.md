# üß™ Guide de Test Unifi√© - Mod√®les Apertus

Un seul script pour tous vos besoins de test: **`test.py`**

---

## üìã Syntaxe G√©n√©rale

```bash
python test.py [--model <chemin> | --base] [--question <texte> | --benchmark] [--compare] [--output <fichier>]
```

---

## üéØ Cas d'Usage

### **1. Question Libre sur Mod√®le Fine-tun√©**

```bash
python test.py --model Apertus-FT/output/apertus_lora_custom_001 --question "Qu'est-ce que le RGPD?"
```

### **2. Question Libre sur Mod√®le de Base**

```bash
python test.py --base --question "Qu'est-ce que le RGPD?"
```

### **3. Benchmark Pr√©d√©fini (9 questions) sur Mod√®le Fine-tun√©**

```bash
python test.py --model Apertus-FT/output/apertus_lora_custom_001 --benchmark
```

**Sauvegarde automatique dans:** `benchmark_YYYYMMDD_HHMMSS.txt`

### **4. Benchmark sur Mod√®le de Base**

```bash
python test.py --base --benchmark --output benchmark_base.txt
```

### **5. Comparaison BASE vs Fine-tun√© avec Question Libre** ‚≠ê

```bash
python test.py --model Apertus-FT/output/apertus_lora_custom_001 --compare --question "Explique-moi LoRA"
```

### **6. Comparaison BASE vs Fine-tun√© avec Benchmark Complet** ‚≠ê‚≠ê‚≠ê

```bash
python test.py --model Apertus-FT/output/apertus_lora_custom_001 --compare --benchmark
```

---

## üìä Les 9 Questions du Benchmark

| # | Cat√©gorie | Sujet |
|---|-----------|-------|
| 1 | Logique | Syllogisme (chats et animaux) |
| 2 | Math√©matiques | Probl√®me de trains |
| 3 | Analyse Critique | IA en m√©decine (avantages/inconv√©nients) |
| 4 | Cr√©ativit√© | Histoire de science-fiction |
| 5 | Technique | LoRA vs Fine-tuning complet |
| 6 | √âthique | Dilemme du tramway (voiture autonome) |
| 7 | RGPD | Droit √† l'effacement et ML (niveau DPO) |
| 8 | Programmation | Crible d'√âratosth√®ne en Python |
| 9 | Multilinguisme | Supervised vs Unsupervised (EN + FR) |

---

## üîß Options Avanc√©es

### **Contr√¥ler la Longueur des R√©ponses**

Par d√©faut: **4096 tokens** (permet des r√©ponses compl√®tes et d√©taill√©es)

Pour des r√©ponses encore plus longues:
```bash
./test.sh --model <chemin> --question "..." --max-tokens 8192
```

Pour des r√©ponses plus courtes (plus rapide):
```bash
./test.sh --model <chemin> --question "..." --max-tokens 1024
```

### **Sauvegarder les R√©sultats**

```bash
python test.py --model <chemin> --benchmark --output mes_resultats.txt
```

---

## üí° Workflows Recommand√©s

### **Workflow 1: Test Rapide d'un Nouveau Fine-tuning**

```bash
# 1. Test avec une question simple
python test.py --model Apertus-FT/output/nouveau_modele --question "Bonjour, qui es-tu?"

# 2. Si satisfait, comparaison avec BASE
python test.py --model Apertus-FT/output/nouveau_modele --compare --question "Explique le fine-tuning"

# 3. Si toujours satisfait, benchmark complet
python test.py --model Apertus-FT/output/nouveau_modele --benchmark
```

### **Workflow 2: √âvaluation Compl√®te pour Production**

```bash
# Comparaison compl√®te BASE vs Fine-tun√©
python test.py --model Apertus-FT/output/mon_modele --compare --benchmark

# R√©sultats affich√©s √† l'√©cran pour analyse imm√©diate
```

### **Workflow 3: Comparer Deux Configurations de Fine-tuning**

```bash
# Benchmark du mod√®le 1
python test.py --model Apertus-FT/output/config1 --benchmark --output results_config1.txt

# Benchmark du mod√®le 2
python test.py --model Apertus-FT/output/config2 --benchmark --output results_config2.txt

# Comparer manuellement les deux fichiers
diff results_config1.txt results_config2.txt
```

### **Workflow 4: Tester sur Vos Propres Questions**

```bash
# Question 1
python test.py --model <chemin> --compare --question "Question sp√©cifique √† votre domaine 1"

# Question 2
python test.py --model <chemin> --compare --question "Question sp√©cifique √† votre domaine 2"

# etc.
```

---

## üìà √âvaluation des R√©sultats

Apr√®s avoir lanc√© un benchmark, comparez avec `reference_answers.md`:

```bash
# 1. Lancer le benchmark
python test.py --model <chemin> --benchmark --output my_results.txt

# 2. Ouvrir c√¥te √† c√¥te
# - my_results.txt (r√©ponses du mod√®le)
# - reference_answers.md (r√©ponses attendues)

# 3. Noter selon la grille (voir reference_answers.md)
```

**Grille de notation:**
- **90-100:** Expert - Pr√™t pour production
- **75-89:** Avanc√© - Bon pour la plupart des cas
- **60-74:** Interm√©diaire - N√©cessite am√©liorations
- **45-59:** D√©butant - R√©entra√Ænement recommand√©
- **<45:** Insuffisant - Revoir la strat√©gie

---

## üéì Exemples Concrets

### **Exemple 1: Premier test d'un mod√®le**

```bash
python test.py --model Apertus-FT/output/apertus_lora_custom_001 \
    --question "Explique-moi en 3 phrases ce qu'est le fine-tuning"
```

### **Exemple 2: V√©rifier l'impact du fine-tuning**

```bash
python test.py --model Apertus-FT/output/apertus_lora_custom_001 \
    --compare \
    --question "Quels sont les principaux d√©fis de l'IA aujourd'hui?"
```

### **Exemple 3: √âvaluation compl√®te**

```bash
python test.py --model Apertus-FT/output/apertus_lora_custom_001 \
    --compare \
    --benchmark
```

**Dur√©e estim√©e:** ~30-45 minutes (9 questions √ó 2 mod√®les)

### **Exemple 4: Benchmark du mod√®le de base (r√©f√©rence)**

```bash
python test.py --base --benchmark --output benchmark_base_reference.txt
```

---

## üÜò Aide et Exemples

Pour voir tous les exemples d'utilisation:

```bash
python test.py --help
```

---

## üìÅ Fichiers G√©n√©r√©s

```
apertus-finetuning-recipes/
‚îú‚îÄ‚îÄ benchmark_YYYYMMDD_HHMMSS.txt    # R√©sultats automatiques
‚îú‚îÄ‚îÄ my_results.txt                    # R√©sultats personnalis√©s (--output)
‚îî‚îÄ‚îÄ reference_answers.md              # R√©ponses de r√©f√©rence pour √©valuation
```

---

## ‚ö° R√©sum√© des Commandes Essentielles

| Action | Commande |
|--------|----------|
| **Question libre sur fine-tun√©** | `python test.py --model <chemin> --question "..."` |
| **Question libre sur BASE** | `python test.py --base --question "..."` |
| **Benchmark fine-tun√©** | `python test.py --model <chemin> --benchmark` |
| **Comparer avec question** | `python test.py --model <chemin> --compare --question "..."` |
| **Comparer avec benchmark** ‚≠ê | `python test.py --model <chemin> --compare --benchmark` |

---

## üîç Diff√©rences avec l'Ancien Syst√®me

### **Avant (multiple scripts):**
- ‚ùå `test_model.py` pour fine-tun√©
- ‚ùå `test_base_model.py` pour base
- ‚ùå `compare_models.sh` pour comparaison
- ‚ùå `benchmark_model.sh` pour benchmark
- ‚ùå `quick_test.sh` pour tests interactifs

### **Maintenant (un seul script):**
- ‚úÖ `test.py` pour TOUT
- ‚úÖ Syntaxe coh√©rente et unifi√©e
- ‚úÖ Plus simple √† utiliser
- ‚úÖ Plus facile √† maintenir

---

## üí° Conseils

1. **Toujours commencer par une question libre** pour v√©rifier que le mod√®le fonctionne
2. **Utiliser --compare** pour mesurer l'impact r√©el du fine-tuning
3. **Le benchmark complet prend du temps** (~30-45 min) - r√©servez-le pour l'√©valuation finale
4. **Sauvegarder les r√©sultats** avec --output pour comparaisons futures
5. **Comparer avec reference_answers.md** pour une √©valuation objective

---

## üöÄ Quick Start

```bash
# Test le plus simple
python test.py --model Apertus-FT/output/apertus_lora_custom_001 --question "Bonjour!"

# Test le plus complet
python test.py --model Apertus-FT/output/apertus_lora_custom_001 --compare --benchmark
```

C'est tout! Un seul script, toutes les fonctionnalit√©s. üéØ
